# Code Cleanup Tasks


## llm.py

the LLM helper should accept and return a list of structured objects, not a dict. e.g.

@dataclass
class ToolCall:
  tool_name: str
  tool_args: dict[str, Any]
  tool_result: Any

enum Role:
  SYSTEM
  USER
  ASSISTANT
  TOOL

@dataclass
class LLMMessage:
  role: str
  content: str
  tool_calls: list[ToolCall]


@dataclass
class LLMResponse:
  messages: list[LLMMessage]
  tool_calls: list[ToolCall]
  error_message: str | None = None
  response_time_ms: int = 0
  raw_response: dict | None = None


The completion() call should take a list of LLMMessage objects or a single string, and return a LLMResponse object.
Then update @evaluation_simple.py to use the new LLMResponse object.


## evaluation framework

References:

@evaluation_simple.py @tidyllm/evaluation.py

The evaluation framework should be simple to use.
An evalution is a file with a set of test functions in it.
We run all of the test functions, and report back a tally of results.
The expectation is that some test cases will fail, and we expect good logging for which tests failed and why.

The "if __main__" block should look like:

test_cases = evaluation.find_test_cases(current_module)
runner = EvaluationRunner(test_cases)
runner.main()

The runner main creates a Click CLI with flags to filter the test cases by name.
After all the tests run summary statistics are printed out.
Tests should optionally run in parallel